@article{Huang2019,
   abstract = {Facial Expression Recognition (FER), as the primary processing method for non-verbal intentions, is an important and promising field of computer vision and artificial intelligence, and one of the subject areas of symmetry. This survey is a comprehensive and structured overview of recent advances in FER.We first categorise the existing FER methods into two main groups, i.e., conventional approaches and deep learning-based approaches. Methodologically, to highlight the differences and similarities, we propose a general framework of a conventional FER approach and review the possible technologies that can be employed in each component. As for deep learning-based methods, four kinds of neural network-based state-of-the-art FER approaches are presented and analysed. Besides, we introduce seventeen commonly used FER datasets and summarise four FER-related elements of datasets that may influence the choosing and processing of FER approaches. Evaluation methods and metrics are given in the later part to show how to assess FER algorithms, along with subsequent performance comparisons of different FER approaches on the benchmark datasets. At the end of the survey, we present some challenges and opportunities that need to be addressed in future.},
   author = {Yunxin Huang and Fei Chen and Shaohe Lv and Xiaodong Wang},
   doi = {10.3390/sym11101189},
   issn = {20738994},
   issue = {10},
   journal = {Symmetry},
   title = {Facial expression recognition: A survey},
   volume = {11},
   year = {2019},
}
@article{Wang2020,
   abstract = {The convolutional neural network (CNN) has been widely used in image recognition field due to its good performance. This paper proposes a facial expression recognition method based on the CNN model. Regarding the complexity of the hierarchic structure of the CNN model, the activation function is its core, because the nonlinear ability of the activation function really makes the deep neural network have authentic artificial intelligence. Among common activation functions, the ReLu function is one of the best of them, but it also has some shortcomings. Since the derivative of the ReLu function is always zero when the input value is negative, it is likely to appear as the phenomenon of neuronal necrosis. In order to solve the above problem, the influence of the activation function in the CNN model is studied in this paper. According to the design principle of the activation function in CNN model, a new piecewise activation function is proposed. Five common activation functions (i.e., sigmoid, tanh, ReLu, leaky ReLus and softplus-ReLu, plus the new activation function) have been analysed and compared in facial expression recognition tasks based on the Keras framework. The Experimental results on two public facial expression databases (i.e., JAFFE and FER2013) show that the convolutional neural network based on the improved activation function has a better performance than most-of-the-art activation functions.},
   author = {Yingying Wang and Yibin Li and Yong Song and Xuewen Rong},
   doi = {10.3390/app10051897},
   issn = {20763417},
   issue = {5},
   journal = {Applied Sciences (Switzerland)},
   title = {The influence of the activation function in a convolution neural network model of facial expression recognition},
   volume = {10},
   year = {2020},
}
@article{Minaee2021,
   abstract = {Facial expression recognition has been an active area of research over the past few decades, and it is still challenging due to the high intra-class variation. Traditional approaches for this problem rely on hand-crafted features such as SIFT, HOG, and LBP, followed by a classifier trained on a database of images or videos. Most of these works perform reasonably well on datasets of images captured in a controlled condition but fail to perform as well on more challenging datasets with more image variation and partial faces. In recent years, several works proposed an end-to-end framework for facial expression recognition using deep learning models. Despite the better performance of these works, there are still much room for improvement. In this work, we propose a deep learning approach based on attentional convolutional network that is able to focus on important parts of the face and achieves significant improvement over previous models on multiple datasets, including FER-2013, CK+, FERG, and JAFFE. We also use a visualization technique that is able to find important facial regions to detect different emotions based on the classifier’s output. Through experimental results, we show that different emotions are sensitive to different parts of the face.},
   author = {Shervin Minaee and Mehdi Minaei and Amirali Abdolrashidi},
   doi = {10.3390/s21093046},
   issn = {14248220},
   issue = {9},
   journal = {Sensors},
   title = {Deep-emotion: Facial expression recognition using attentional convolutional network},
   volume = {21},
   year = {2021},
}
@article{Li2020,
   abstract = {With the transition of facial expression recognition (FER) from laboratory-controlled to in-the-wild conditions and the recent success of deep learning in various fields, deep neural networks have increasingly been leveraged to learn discriminative representations for automatic FER. Recent deep FER systems generally focus on two important issues: overfitting caused by a lack of sufficient training data and expression-unrelated variations, such as illumination, head pose and identity bias. In this survey, we provide a comprehensive review on deep FER, including datasets and algorithms that provide insights into these problems. First, we introduce available datasets that are widely used and provide data selection and evaluation principles. We then describe the standard pipeline of a deep FER system with related background knowledge and suggestions of applicable implementations. For the state of the art in deep FER, we introduce existing deep networks and training strategies that are designed for FER, and discuss their advantages and limitations. Competitive performances and experimental comparisons on widely used benchmarks are also summarized. We then extend our survey to additional related issues and application scenarios. Finally, we review the remaining challenges and opportunities in this field as well as future directions for the design of robust deep FER system.},
   author = {Shan Li and Weihong Deng},
   doi = {10.1109/TAFFC.2020.2981446},
   issn = {19493045},
   journal = {IEEE Transactions on Affective Computing},
   title = {Deep Facial Expression Recognition: A Survey},
   year = {2020},
}
@working_paper{Ionescu2013,
   abstract = {In this paper we propose a novel computer vision method for classifying human facial expression from low resolution images. Our method uses the bag of words representa- tion. It extracts dense SIFT descriptors ei- ther from the whole image or from a spa- tial pyramid that divides the image into in- creasingly fine sub-regions. Then, it repre- sents images as normalized (spatial) presence vectors of visual words from a codebook ob- tained through clustering image descriptors. Linear kernels are built for several choices of spatial presence vectors, and combined into weighted sums for multiple kernel learning (MKL). For machine learning, the method makes use of multi-class one-versus-all SVM on the MKL kernel computed using this rep- resentation, but with an important twist, the learning is local, as opposed to global – in the sense that, for each face with an unknown la- bel, a set of neighbors is selected to build a local classification model, which is eventually used to classify only that particular face.},
   author = {Radu Tudor Ionescu and Marius Popescu and Cristian Grozea},
   institution = {ICML},
   title = {Local Learning to Improve Bag of Visual Words Model for Facial Expression Recognition},
   year = {2013},
}
@article{Tang2013,
   abstract = {Recently, fully-connected and convolutional neural networks have been trained to achieve state-of-the-art performance on a wide variety of tasks such as speech recognition, image classification, natural language processing, and bioinformatics. For classification tasks, most of these "deep learning" models employ the softmax activation function for prediction and minimize cross-entropy loss. In this paper, we demonstrate a small but consistent advantage of replacing the softmax layer with a linear support vector machine. Learning minimizes a margin-based loss instead of the cross-entropy loss. While there have been various combinations of neural nets and SVMs in prior art, our results using L2-SVMs show that by simply replacing softmax with linear SVMs gives significant gains on popular deep learning datasets MNIST, CIFAR-10, and the ICML 2013 Representation Learning Workshop's face expression recognition challenge.},
   author = {Yichuan Tang},
   month = {6},
   title = {Deep Learning using Linear Support Vector Machines},
   year = {2013},
}
@working_paper{Leslie2020,
   abstract = {Over the past couple of years, the growing debate around automated facial recognition has reached a boiling point. As developers have continued to swiftly expand the scope of these kinds of technologies into an almost unbounded range of applications, an increasingly strident chorus of critical voices has sounded concerns about the injurious effects of the proliferation of such systems on impacted individuals and communities. Opponents argue that the irresponsible design and use of facial detection and recognition technologies (FDRTs) threatens to violate civil liberties, infringe on basic human rights and further entrench structural racism and systemic marginalisation. They also caution that the gradual creep of face surveillance infrastructures into every domain of lived experience may eventually eradicate the modern democratic forms of life that have long provided cherished means to individual flourishing, social solidarity and human self-creation. Defenders, by contrast, emphasise the gains in public safety, security and efficiency that digitally streamlined capacities for facial identification, identity verification and trait characterisation may bring. In this explainer, I focus on one central aspect of this debate: the role that dynamics of bias and discrimination play in the development and deployment of FDRTs. I examine how historical patterns of discrimination have made inroads into the design and implementation of FDRTs from their very earliest moments. And, I explain the ways in which the use of biased FDRTs can lead to distributional and recognitional injustices. I also describe how certain complacent attitudes of innovators and users toward redressing  these harms raise serious concerns about expanding future adoption. The explainer concludes with an exploration of broader ethical questions around the potential proliferation of pervasive face-based surveillance infrastructures and makes some recommendations for cultivating more responsible approaches to the development and governance of these technologies. },
   author = {David Dr Leslie},
   institution = {The Alan Turing Institute},
   title = {Understanding bias in facial recognition technologies},
   year = {2020},
}
@working_paper{,
   abstract = {Learning Convolutional Neural Networks (CNN) is commonly carried out by plain supervised gradient descent. With sufficient training data, this leads to very competitive results for visual recognition tasks when starting from a random initialization. When the amount of labeled data is limited, CNNs reveal their strong dependence on large amounts of training data. However, recent results have shown that a well chosen optimization starting point can be beneficial for convergence to a good generalizing minimum. This starting point was mostly found using unsupervised feature learning techniques such as sparse coding or transfer learning from related recognition tasks. In this work, we compare these two approaches against a simple patch based initialization scheme and a random initialization of the weights. We show that pre-training helps to train CNNs from few samples and that the correct choice of the initialization scheme can push the network’s performance by up to 41% compared to random initialization.},
   author = {Raimar Wagner and Markus Thom and Roland Schweiger and Günther Palm and Albrecht Rothermel},
   institution = {IEEE Xplore},
   title = {Learning Convolutional Neural Networks From Few Samples},
}
@article{Goodfellow2013,
   abstract = {The ICML 2013 Workshop on Challenges in Representation Learning focused on three challenges: the black box learning challenge, the facial expression recognition challenge, and the multimodal learning challenge. We describe the datasets created for these challenges and summarize the results of the competitions. We provide suggestions for organizers of future challenges and some comments on what kind of knowledge can be gained from machine learning competitions.},
   author = {Ian J. Goodfellow and Dumitru Erhan and Pierre Luc Carrier and Aaron Courville and Mehdi Mirza and Ben Hamner and Will Cukierski and Yichuan Tang and David Thaler and Dong-Hyun Lee and Yingbo Zhou and Chetan Ramaiah and Fangxiang Feng and Ruifan Li and Xiaojie Wang and Dimitris Athanasakis and John Shawe-Taylor and Maxim Milakov and John Park and Radu Ionescu and Marius Popescu and Cristian Grozea and James Bergstra and Jingjing Xie and Lukasz Romaszko and Bing Xu and Zhang Chuang and Yoshua Bengio},
   month = {7},
   title = {Challenges in Representation Learning: A report on three machine learning contests},
   year = {2013},
}
